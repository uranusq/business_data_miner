[general]
database = "prod.db"    # Location of SQLite database

[common]
use = true               # Use this crawler or not
path = "data/common"     # Where collected data will be saved
debug = false            # Show debug output
timeout = 30             # Query to Common Crawl Index API may take time
search_interval = 2      # In seconds. Do not overload Index API server
crawl_db = "CC-MAIN-2019-22" # Web Archive version 
wait_time = 53           # In milliseconds. Wait time between loads from Amazon S3. If too fast may DoS
workers = 40             # Number of goroutines (threads) for this crawling method

[google]
use = true
path = "data/google"
debug = false
extension = "pdf"       # Which files to search
search_interval = 30    # In seconds
max_file_size = 35      # In megabytes
workers = 40

[colly]
use = true
path = "data/colly"
debug = false
max_file_size = 35      # In megabytes
max_html_load = 50      # In megabytes
work_minutes = 30
workers = 20